{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e10c41",
   "metadata": {},
   "source": [
    "# Lab 7: Deep Learning for Multiclass Classification\n",
    "## Multiclass Classification for Covertype Dataset\n",
    "This notebook demonstrates a step-by-step implementation of a neural network for multiclass classification using the Covertype dataset. The goal is to classify forest cover types based on input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecdc7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 14:05:03.321085: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-08 14:05:03.328755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-08 14:05:03.337755: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-08 14:05:03.340357: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-08 14:05:03.347412: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-08 14:05:03.745735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f174bb7",
   "metadata": {},
   "source": [
    "## Step 1: Load and Preprocess the Data\n",
    "We load the Covertype dataset, split it into training and test sets, standardize the features, and one-hot encode the target labels for multiclass classification.\n",
    "\n",
    "Classes: 7\n",
    "<br/>Samples total: 581012\n",
    "<br/>Features: 54\n",
    "<br/>Features type:int\n",
    "\n",
    "1. Elevation / quantitative /meters / Elevation in meters\n",
    "2. Aspect / quantitative / azimuth / Aspect in degrees azimuth\n",
    "3. Slope / quantitative / degrees / Slope in degrees\n",
    "4. Horizontal_Distance_To_Hydrology / quantitative / meters / Horz Dist to nearest surface water features\n",
    "5. Vertical_Distance_To_Hydrology / quantitative / meters / Vert Dist to nearest surface water features\n",
    "6. Horizontal_Distance_To_Roadways / quantitative / meters / Horz Dist to nearest roadway\n",
    "7. Hillshade_9am / quantitative / 0 to 255 index / Hillshade index at 9am, summer solstice\n",
    "8. Hillshade_Noon / quantitative / 0 to 255 index / Hillshade index at noon, summer soltice\n",
    "9. Hillshade_3pm / quantitative / 0 to 255 index / Hillshade index at 3pm, summer solstice\n",
    "10. Horizontal_Distance_To_Fire_Points / quantitative / meters / Horz Dist to nearest wildfire ignition points\n",
    "11. Wilderness_Area (4 binary columns) / qualitative / 0 (absence) or 1 (presence) / Wilderness area designation\n",
    "12. Soil_Type (40 binary columns) / qualitative / 0 (absence) or 1 (presence) / Soil Type designation\n",
    "13. Cover_Type (7 types) / integer / 1 to 7 / Forest Cover Type designation\n",
    "<br/>Target: Cover_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b2a328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Covertype dataset to a pandas DataFrame(df)\n",
    "# Separate the features(X) and labels(y)\n",
    "\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "# -------------------------------\n",
    "\n",
    "# Convert labels to integers (from 1-7 to 0-6 for classification)\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49018f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type32</th>\n",
       "      <th>Soil_Type33</th>\n",
       "      <th>Soil_Type34</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.368684</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.184681</td>\n",
       "      <td>0.223514</td>\n",
       "      <td>0.071659</td>\n",
       "      <td>0.870079</td>\n",
       "      <td>0.913386</td>\n",
       "      <td>0.582677</td>\n",
       "      <td>0.875366</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.365683</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.151754</td>\n",
       "      <td>0.215762</td>\n",
       "      <td>0.054798</td>\n",
       "      <td>0.866142</td>\n",
       "      <td>0.925197</td>\n",
       "      <td>0.594488</td>\n",
       "      <td>0.867838</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.472736</td>\n",
       "      <td>0.386111</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.191840</td>\n",
       "      <td>0.307494</td>\n",
       "      <td>0.446817</td>\n",
       "      <td>0.921260</td>\n",
       "      <td>0.937008</td>\n",
       "      <td>0.531496</td>\n",
       "      <td>0.853339</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.463232</td>\n",
       "      <td>0.430556</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.173228</td>\n",
       "      <td>0.375969</td>\n",
       "      <td>0.434172</td>\n",
       "      <td>0.937008</td>\n",
       "      <td>0.937008</td>\n",
       "      <td>0.480315</td>\n",
       "      <td>0.865886</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.368184</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.109520</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.054939</td>\n",
       "      <td>0.866142</td>\n",
       "      <td>0.921260</td>\n",
       "      <td>0.590551</td>\n",
       "      <td>0.860449</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation    Aspect     Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0   0.368684  0.141667  0.045455                          0.184681   \n",
       "1   0.365683  0.155556  0.030303                          0.151754   \n",
       "2   0.472736  0.386111  0.136364                          0.191840   \n",
       "3   0.463232  0.430556  0.272727                          0.173228   \n",
       "4   0.368184  0.125000  0.030303                          0.109520   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                        0.223514                         0.071659   \n",
       "1                        0.215762                         0.054798   \n",
       "2                        0.307494                         0.446817   \n",
       "3                        0.375969                         0.434172   \n",
       "4                        0.222222                         0.054939   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0       0.870079        0.913386       0.582677   \n",
       "1       0.866142        0.925197       0.594488   \n",
       "2       0.921260        0.937008       0.531496   \n",
       "3       0.937008        0.937008       0.480315   \n",
       "4       0.866142        0.921260       0.590551   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points  ...  Soil_Type32  Soil_Type33  \\\n",
       "0                            0.875366  ...            0            0   \n",
       "1                            0.867838  ...            0            0   \n",
       "2                            0.853339  ...            0            0   \n",
       "3                            0.865886  ...            0            0   \n",
       "4                            0.860449  ...            0            0   \n",
       "\n",
       "   Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  Soil_Type38  \\\n",
       "0            0            0            0            0            0   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   Soil_Type39  Soil_Type40  target  \n",
       "0            0            0       5  \n",
       "1            0            0       5  \n",
       "2            0            0       2  \n",
       "3            0            0       2  \n",
       "4            0            0       5  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first 5 rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fc1db",
   "metadata": {},
   "source": [
    "## Split and Scale the Dataset\n",
    "We split the dataset into training and test sets with a test size of 20% and scale the features using the StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d085e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets using 80% training and 20% testing\n",
    "# Set random_state to 42 for reproducibility\n",
    "# Use train_test_split function from scikit-learn\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "# -------------------------------\n",
    "\n",
    "# Standardize the feature data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c058f",
   "metadata": {},
   "source": [
    "## Encode the Target Labels\n",
    "We one-hot encode the target labels using the to_categorical function from the keras.utils module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c762ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels using to_categorical function from Keras\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "# -------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e88bd",
   "metadata": {},
   "source": [
    "## Step 2: Build the Model\n",
    "We define a neural network with multiple dense layers and dropout layers to prevent overfitting. The output layer uses a softmax activation for multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b90891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirawichvac/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736319905.031172 3364130 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1736319905.051923 3364130 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1736319905.052065 3364130 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1736319905.052774 3364130 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1736319905.052849 3364130 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1736319905.052897 3364130 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1736319905.090811 3364130 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1736319905.090918 3364130 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1736319905.090974 3364130 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-08 14:05:05.091026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22357 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "# Complete the code to build a Sequential model\n",
    "# The model should have 4 Dense layers with 256, 128, 64, and 7 units\n",
    "# Use 'relu' activation function for the first 3 layers and 'softmax' for the last layer\n",
    "# Add Dropout layers with 0.3 dropout rate after the first 2 Dense layers\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(..., activation=..., input_shape=(...,)),\n",
    "    Dropout(...),\n",
    "    Dense(..., activation=...),\n",
    "    Dropout(...),\n",
    "    Dense(..., activation=...),\n",
    "    Dense(..., activation=...)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4a532",
   "metadata": {},
   "source": [
    "## Step 3: Compile the Model\n",
    "We compile the model using the Adam optimizer with a learning rate of 0.001, categorical crossentropy loss, and accuracy as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba7477a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# Use Adam optimizer with learning rate of 0.001\n",
    "\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79549c5f",
   "metadata": {},
   "source": [
    "## Step 4: Train the Model\n",
    "We train the model on the training data for 20 epochs, using a batch size of 64 and validating on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6be44067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736319906.140903 3364227 service.cc:146] XLA service 0x72439c01d830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1736319906.140918 3364227 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2025-01-08 14:05:06.155596: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-01-08 14:05:06.228735: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2025-01-08 14:05:06.307893: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:762] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  319/14526\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 475us/step - accuracy: 0.5911 - loss: 1.0363"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736319907.102871 3364227 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 675us/step - accuracy: 0.7247 - loss: 0.6464 - val_accuracy: 0.8049 - val_loss: 0.4654\n",
      "Epoch 2/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 526us/step - accuracy: 0.7869 - loss: 0.5038 - val_accuracy: 0.8251 - val_loss: 0.4211\n",
      "Epoch 3/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 538us/step - accuracy: 0.8038 - loss: 0.4644 - val_accuracy: 0.8383 - val_loss: 0.3933\n",
      "Epoch 4/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 532us/step - accuracy: 0.8142 - loss: 0.4440 - val_accuracy: 0.8482 - val_loss: 0.3681\n",
      "Epoch 5/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 528us/step - accuracy: 0.8191 - loss: 0.4315 - val_accuracy: 0.8500 - val_loss: 0.3640\n",
      "Epoch 6/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 521us/step - accuracy: 0.8234 - loss: 0.4228 - val_accuracy: 0.8601 - val_loss: 0.3477\n",
      "Epoch 7/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 532us/step - accuracy: 0.8282 - loss: 0.4135 - val_accuracy: 0.8635 - val_loss: 0.3364\n",
      "Epoch 8/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 530us/step - accuracy: 0.8309 - loss: 0.4073 - val_accuracy: 0.8677 - val_loss: 0.3294\n",
      "Epoch 9/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 537us/step - accuracy: 0.8337 - loss: 0.4006 - val_accuracy: 0.8704 - val_loss: 0.3216\n",
      "Epoch 10/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 519us/step - accuracy: 0.8368 - loss: 0.3943 - val_accuracy: 0.8725 - val_loss: 0.3205\n",
      "Epoch 11/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 530us/step - accuracy: 0.8396 - loss: 0.3904 - val_accuracy: 0.8763 - val_loss: 0.3126\n",
      "Epoch 12/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 529us/step - accuracy: 0.8400 - loss: 0.3869 - val_accuracy: 0.8758 - val_loss: 0.3071\n",
      "Epoch 13/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 529us/step - accuracy: 0.8423 - loss: 0.3848 - val_accuracy: 0.8800 - val_loss: 0.3054\n",
      "Epoch 14/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 536us/step - accuracy: 0.8442 - loss: 0.3800 - val_accuracy: 0.8793 - val_loss: 0.3033\n",
      "Epoch 15/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 532us/step - accuracy: 0.8454 - loss: 0.3773 - val_accuracy: 0.8821 - val_loss: 0.2997\n",
      "Epoch 16/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 526us/step - accuracy: 0.8467 - loss: 0.3762 - val_accuracy: 0.8828 - val_loss: 0.3029\n",
      "Epoch 17/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 534us/step - accuracy: 0.8483 - loss: 0.3714 - val_accuracy: 0.8822 - val_loss: 0.2957\n",
      "Epoch 18/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 534us/step - accuracy: 0.8489 - loss: 0.3711 - val_accuracy: 0.8822 - val_loss: 0.2969\n",
      "Epoch 19/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 535us/step - accuracy: 0.8502 - loss: 0.3686 - val_accuracy: 0.8856 - val_loss: 0.2893\n",
      "Epoch 20/20\n",
      "\u001b[1m14526/14526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 541us/step - accuracy: 0.8499 - loss: 0.3676 - val_accuracy: 0.8841 - val_loss: 0.2920\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# Use 50 epochs and batch size of 32\n",
    "# Use the training and test sets\n",
    "# Save the training history to a variable(history)\n",
    "\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a50c09",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the Model\n",
    "We evaluate the model on the test set and print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b04d2ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3632/3632 - 1s - 311us/step - accuracy: 0.8841 - loss: 0.2920\n",
      "Test accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8df155",
   "metadata": {},
   "source": [
    "## Step 6: Make Predictions\n",
    "We use the trained model to make predictions for the first 10 samples in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f020c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487ms/step\n",
      "\n",
      " Predicted Probabilities:\n",
      "Class 0   Class 1   Class 2   Class 3   Class 4   Class 5   Class 6   \n",
      "0.907     0.00306   4.13e-17  3.96e-22  4.69e-06  2.53e-18  0.0896    \n",
      "0.21      0.774     0.00272   0.000152  0.00491   0.00626   0.00257   \n",
      "0.00751   0.906     0.00867   5.33e-06  0.0249    0.0532    1.34e-05  \n",
      "0.0434    0.957     2.68e-13  4.85e-15  1.78e-06  9.85e-11  8.59e-08  \n",
      "4.71e-06  0.908     3.09e-05  2.79e-24  0.0919    4.15e-07  3.02e-08  \n",
      "4e-15     3.52e-06  0.995     2.93e-06  5.7e-10   0.00504   8.72e-34  \n",
      "0.0626    0.937     6.56e-17  4.4e-20   1.03e-07  2.68e-13  2.97e-08  \n",
      "0.994     0.00333   3.43e-13  1.22e-14  7.85e-06  2.08e-13  0.00241   \n",
      "0.0888    0.911     1.09e-12  4.05e-22  7.03e-06  6.09e-11  1.63e-05  \n",
      "0.00226   0.991     1.28e-07  2.02e-09  0.00711   1.57e-05  1.88e-08  \n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "# Use the first 10 test data points to make predictions(predictions)\n",
    "\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "# -------------------------------\n",
    "\n",
    "# Show the predicted probabilities\n",
    "print('\\n Predicted Probabilities:')\n",
    "print(f\"{'Class 0':<10}{'Class 1':<10}{'Class 2':<10}{'Class 3':<10}{'Class 4':<10}{'Class 5':<10}{'Class 6':<10}\")\n",
    "for pred_prob in predictions:\n",
    "    print(f\"{pred_prob[0]:<10.3}{pred_prob[1]:<10.3}{pred_prob[2]:<10.3}{pred_prob[3]:<10.3}{pred_prob[4]:<10.3}{pred_prob[5]:<10.3}{pred_prob[6]:<10.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72bc3d1",
   "metadata": {},
   "source": [
    "## Step 7: Interpret the Predictions\n",
    "We convert the predicted probabilities into class labels using `np.argmax`, and compare them with the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c22717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [0 1 1 1 1 2 1 0 1 1]\n",
      "True labels: [0 1 1 1 1 2 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Interpret the predictions\n",
    "# Use np.argmax() to get the predicted class labels from the predicted probabilities\n",
    "\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "# Show the predicted and true labels\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "print(\"True labels:\", true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b945a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
